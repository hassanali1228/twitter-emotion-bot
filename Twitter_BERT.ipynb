{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\r\n",
    "import math\r\n",
    "import datetime\r\n",
    "from tqdm import tqdm\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import tensorflow as tf\r\n",
    "#print(tf._version_)\r\n",
    "import tensorflow  as keras\r\n",
    "\r\n",
    "import bert as bert\r\n",
    "from bert import BertModelLayer\r\n",
    "from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\r\n",
    "from bert.tokenization.bert_tokenization import FullTokenizer\r\n",
    "import seaborn as sns\r\n",
    "from pylab import rcParams\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from matplotlib.ticker import MaxNLocator\r\n",
    "from matplotlib import rc\r\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train  = pd.read_csv('./sample_data/combined.csv')\r\n",
    "test = pd.read_csv('./sample_data/combined.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "chart = sns.countplot(train.Column3)\r\n",
    "plt.title(\"Number of examples per Categories\")\r\n",
    "chart.set_xticklabels(chart.get_xticklabels(),rotation = 30,horizontalalignment='right')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\r\n",
    "!unzip uncased_L-12_H-768_A-12.zip"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "os.makedirs('model',exist_ok=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!mv uncased_L-12_H-768_A-12/ model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "bert_model_name = 'uncased_L-12_H-768_A-12'\r\n",
    "bert_ckpt_dir = os.path.join('model/',bert_model_name)\r\n",
    "bert_ckpt_file = os.path.join(bert_ckpt_dir,'bert_model.cpkt.index')\r\n",
    "bert_config_file = os.path.join(bert_ckpt_dir,'bert_config.json')\r\n",
    "\r\n",
    "bert_ckpt_dir"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'model/uncased_L-12_H-768_A-12'"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class sentiment:\r\n",
    "  data_column = 'Column2'\r\n",
    "  label_column = 'Column3'\r\n",
    "\r\n",
    "  def __init__(self,train,test,tokenizer: FullTokenizer,classes, max_seq_len = 192):\r\n",
    "    self.tokenizer=tokenizer\r\n",
    "    self.max_seq_len = 0\r\n",
    "    self.classes = classes\r\n",
    "\r\n",
    "    ((self.train_x,self.train_y),(self.test_x,self.test_y))=\\\r\n",
    "      map(self._prepare,[train,test])\r\n",
    "\r\n",
    "    self.max_seq_len = min(self.max_seq_len,max_seq_len)\r\n",
    "    self.train_x,self.test_x= map(self._pad,[self.train_x,self.test_x])\r\n",
    "\r\n",
    "\r\n",
    "  def _prepare(self,df):\r\n",
    "    x,y = [],[]\r\n",
    "    for _, row in tqdm(df.iterrows()):\r\n",
    "      text,label =\\\r\n",
    "        row[sentiment.data_column],row[sentiment.label_column]\r\n",
    "\r\n",
    "      tokens = self.tokenizer.tokenize(text)\r\n",
    "      tokens = [\"[CLS]\"]+tokens+[\"[SEP]\"]\r\n",
    "\r\n",
    "      token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\r\n",
    "\r\n",
    "      self.max_seq_len = max(self.max_seq_len,len(token_ids))\r\n",
    "\r\n",
    "      x.append(token_ids)\r\n",
    "      y.append(self.classes.index(label))\r\n",
    "      \r\n",
    "    return np.array(x),np.array(y)\r\n",
    "\r\n",
    "\r\n",
    "  def _pad(self,ids):\r\n",
    "    x = []\r\n",
    "\r\n",
    "    for input_ids in ids:\r\n",
    "      cut_point=min(len(input_ids),self.max_seq_len -2)\r\n",
    "      input_ids = input_ids[:cut_point]\r\n",
    "      input_ids = input_ids + [0] * (self.max_seq_len - len(input_ids))\r\n",
    "      x.append(np.array(input_ids))\r\n",
    "\r\n",
    "    return np.array(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokenizer.tokenize(\"I Can't wait to go!\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokens = tokenizer.tokenize(\"I Can't wait to go!\")\r\n",
    "tokenizer.convert_tokens_to_ids(tokens)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def create_model(max_seq_len, bert_ckpt_file):\r\n",
    "  with tf.io.gfile.GFile(bert_config_file, \"r\") as reader:\r\n",
    "      bc = StockBertConfig.from_json_string(reader.read())\r\n",
    "      bert_params = map_stock_config_to_params(bc)\r\n",
    "      bert_params.adapter_size = None\r\n",
    "      bert = BertModelLayer.from_params(bert_params, name=\"bert\")\r\n",
    "  input_ids = tf.keras.layers.Input(\r\n",
    "    shape=(max_seq_len, ),\r\n",
    "    dtype='int32',\r\n",
    "    name=\"input_ids\"\r\n",
    "  )\r\n",
    "  bert_output = bert(input_ids)\r\n",
    "  print(\"bert shape\", bert_output.shape)\r\n",
    "  cls_out = tf.keras.layers.Lambda(lambda seq: seq[:, 0, :])(bert_output)\r\n",
    "  cls_out = tf.keras.layers.Dropout(0.5)(cls_out)\r\n",
    "  logits = tf.keras.layers.Dense(units=768, activation=\"tanh\")(cls_out)\r\n",
    "  logits = tf.keras.layers.Dropout(0.5)(logits)\r\n",
    "  logits = tf.keras.layers.Dense(\r\n",
    "    units=len(classes),\r\n",
    "    activation=\"softmax\"\r\n",
    "  )(logits)\r\n",
    "  model = tf.keras.Model(inputs=input_ids, outputs=logits)\r\n",
    "  model.build(input_shape=(None, max_seq_len))\r\n",
    "  load_stock_weights(bert, bert_ckpt_file)\r\n",
    "  return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "classes = train.Column3.unique().tolist()\r\n",
    "data = sentiment(train,test,tokenizer,classes,max_seq_len=768)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = create_model(data.max_seq_len,'model/uncased_L-12_H-768_A-12/bert_model.ckpt' )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data.train_x.shape\r\n",
    "model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.compile(\r\n",
    "    optimizer = tf.keras.optimizers.Adam(1e-5),loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n",
    "    metrics = [keras.metrics.SparseCategoricalAccuracy(name='acc')]\r\n",
    "\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "log_dir = 'log/sentiment'+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%s\")\r\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\r\n",
    "history = model.fit(\r\n",
    "    x=data.train_x,\r\n",
    "    y=data.train_y,\r\n",
    "    validation_split = 0.1,\r\n",
    "    batch_size = 16,\r\n",
    "    shuffle = True,\r\n",
    "    epochs = 5,\r\n",
    "    callbacks= [tensorboard_callback]\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sentences = ['bad','i hate you']\r\n",
    "\r\n",
    "tokens = map(tokenizer.tokenize,sentences)\r\n",
    "tokens = map(lambda tok:[\"[CLS]\"]+tok+[\"[SEP]\"],tokens)\r\n",
    "\r\n",
    "token_ids = list(map(tokenizer.convert_tokens_to_ids,tokens))\r\n",
    "\r\n",
    "token_ids = map(lambda tids:tids + [0]*(data.max_seq_len-len(tids)),token_ids)\r\n",
    "token_ids = np.array(list(token_ids))\r\n",
    "\r\n",
    "predictions = model.predict(token_ids).argmax(axis=-1)\r\n",
    "for text,label in zip(sentences,predictions):\r\n",
    "  print(\"text:\",text)\r\n",
    "  print(\"intent\",classes[label])"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "f65332f147b41578335efd8d2a3e52675cb60580caf47da42c831203f1235846"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}